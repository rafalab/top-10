Correlation: You are probably doing it wrong
========================================================
author: Rafael A. Irizarry
date: 
autosize: true

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(dslabs)
ds_theme_set()
```

Correlation is not a measure of reproducibility
========================================================
The ENCODE data standards document states that 

>> A typical R2 (Pearson) correlation of gene expression (RPKM) between two biological replicates, for RNAs that are detected in both samples using RPKM or read counts, should be between 0.92 to 0.98. Experiments with biological correlations that fall below 0.9 should be either be repeated or explained.


Simulation
========================================================

```{r}
n <- 1000
x <- rnorm(n, 6, 3.4)
d <- rnorm(n, 0, 1)
y <- x + d
```


Simulation
========================================================

```{r, echo=FALSE}
qplot(x, y)
```


Simulation
========================================================

```{r}
cor(x, y)
```

Simulation
========================================================

```{r}
sd(x-y)
```


Formula
===


$$
\mbox{cor}(X, X+\varepsilon) = \frac{1}{\sqrt{1 + \mbox{var}(\varepsilon)/\mbox{var}(X)}}
$$



Why correlation?
=====

- Why average and standard deviation?

Galton's Data
===

```{r}
library(HistData)
data("GaltonFamilies")
galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)
```

Galton's Data
===

- Supposed we were to summarize these data.

- Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:


Galton's Data
===

```{r, message=FALSE, warning=FALSE}
galton_heights %>% 
  summarize(mean(father), sd(father), mean(son), sd(son))
```

Galton's Data
===

- However, this summary fails to describe an important characteristic of the data: 

```{r echo=FALSE, scatterplot, fig.cap="Heights of father and son pairs plotted against each other."}
galton_heights %>% ggplot(aes(father, son)) + 
  geom_point(alpha = 0.5)
```

Correlation
===

- The correlation coefficient is defined for a list of pairs 
$(x_1, y_1), \dots, (x_n,y_n)$ 
as 

$$
\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)
$$

- with $\mu_x, \mu_y$ the averages of $x_1,\dots, x_n$ and $y_1, \dots, y_n$ respectively and $\sigma_x, \sigma_y$ the standard deviations. 

Correlation
===

- To understand why this equation does in fact summarize how to variables move together consider the $i$-th entry of $x$ is  $\left( \frac{x_i-\mu_x}{\sigma_x} \right)$ SDs away from the average.

- Similarly, the $y_i$, paired that is paired with $x_i$, is $\left( \frac{y_1-\mu_y}{\sigma_y} \right)$ SDs away from the average $y$.

- If $x$ and $y$ are unrelated the product $\left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)$ will be positive ( $ + \tims +$ and $- \times -$ ) as often as negative ($+ \times -$ and $- times +$) and will average out to about 0.


Correlation
===

- This correlation is this average and therefore unrelated variables will have 0 correlation.

- If instead the quantities vary together then we are averaging mostly positive products ( $ + \tims +$ and $- \times -$) and we get a positive correlation.

- If they vary in opposite directions we get a negative correlation. 

Correlation
===

- We can show, mathematically, that the correlation $\rho$ is always between -1 and 1.

$$
\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)^2 = 1/\sigma^2 
\frac{1}{n} \sum_{i=1}^n \left( x_i-\mu_x \right)^2 = 1
$$


Correlation
===

- The correlation between father and son's heights is about 0.5


```{r}
galton_heights %>% summarize(cor(father, son))
```

Correlation
====

```{r, echo=FALSE}
n <- 250
cors <- c(-0.9,-0.5,0,0.5,0.9,0.99)
sim_data <- lapply(cors,function(r) MASS::mvrnorm(n,c(0,0), matrix(c(1,r,r,1),2,2)))
sim_data <- Reduce(rbind, sim_data)
sim_data <- cbind( rep(cors, each=n), sim_data)
colnames(sim_data) <- c("r","x","y")
as.data.frame(sim_data) %>% ggplot(aes(x,y)) +facet_wrap(~r) + geom_point() +geom_vline(xintercept = 0,lty=2) + geom_hline(yintercept = 0,lty=2) 
```
Sample Correlation is a Random Variable
===

- Something to keep in mind when interpreting correlations.

- Also note that because the sample correlation is an average of independent draws, the central limit actually applies, therefore for large enough $N$ the distribution of `R` is approximately normal with expected value $\rho$.

- The standard deviation is somewhat complex to derive but it is $\sqrt{\frac{1-r^2}{N-2}}$.

Anscombe's quartet
===

- Correlation is not always a good summary of the relationship between two variables.

- A famous example used to illustrate this, are the following four artificial datasets, referred to as Anscombe's quartet.



Anscombe's quartet
====

- All these pairs  have a correlation of 0.82:


```{r ,echo=FALSE}
library(tidyr)
anscombe %>% mutate(row = seq_len(n())) %>%
  gather(name, value, -row) %>% 
  separate(name, c("axis", "group"), sep=1) %>%
  spread(axis, value) %>% select(-row) %>%
  ggplot(aes(x,y)) +
  facet_wrap(~group)  +
  geom_smooth(method="lm", fill=NA, fullrange=TRUE, color="blue") +
  geom_point(bg="orange",color="red",cex=3,pch=21)
```

Anscombe's quartet
====

- Correlation is only meaningful in a particular context.

- To help us understand when it is that correlation is meaningful as a summary statistic we will try to predict the son's height using the father's height.

- This will help motivate and define linear regression.

- We start by demonstrating how correlation can be useful for prediction.


Stratification
===

- Suppose we are asked to guess the height of a randomly selected son.

- Because the distribution of son heights is approximately normal, we know the 
average height, `r mean(galton_heights$son)`, is the value with the highest proportion
and would be the prediction with the chances of minimizing the error.

Stratification
===
- But what if we are told
that the father is 72 inches tall, do we sill guess `r mean(galton_heights$son)` for the son? 

- The father is taller than average.

- Specifically, he is 
`r (72 -mean(galton_heights$father))/sd(galton_heights$father)` standard deviations taller than the average father.

Stratification
===

- So should we predict that the son is also `r (72 -mean(galton_heights$father))/sd(galton_heights$father)` standard deviations taller than the average son? 

- It turns
out that this would be an overestimate.

- To see this, we look at all the sons
with fathers who are about 72 inches.

Stratification
===

- We do this by _stratifying_ the
father heights.

- We call this a conditional average since we are computing the average son height _conditioned_ on the father being 72 inches tall. 

Stratification
===

- A challenge when using this approach in practice is that we don't have many fathers that are exactly 72:


```{r}
sum(galton_heights$father == 72)
```

Stratification
===

- If we change the number to 72.5 we only get 


```{r}
sum(galton_heights$father == 72.5)
```

Stratification
===

- This will result in averages with large standard errors and not useful for prediction.

- For now, we will take the approach of creating strata of fathers with very similar heights.

- Specifically, we will round father heights to the nearest inch.

Stratification
===

- This give us the following prediction for the son of a father that is 72 inches tall:


```{r}
conditional_avg <- galton_heights %>% filter(round(father) == 72) %>%
  summarize(avg = mean(son)) %>% .$avg
conditional_avg
```

Stratification
===

- which is `r (conditional_avg - mean(galton_heights$son)) /sd(galton_heights$son)` standard deviations larger than the average son a smaller number than the `r (72 -mean(galton_heights$father))/sd(galton_heights$father)` standard deviations taller that the father was over the the average father.

Stratification
===

- Stratification followed by boxplots lets us see the distribution of each group:


```{r echo=FALSE, boxplot4, fig.cap="Boxplot of son heights stratified by father heights."}
galton_heights %>% mutate(father_strata = factor(round(father))) %>% 
  ggplot(aes(father_strata, son)) + 
  geom_boxplot() + 
  geom_point()
```

Stratification
===

- We can see that the centers of the groups is increasing with height.

- The means of each group appear to follow a linear relationship:


Stratification
===

```{r eval=FALSE, boxplot2, fig.cap="Boxplot of son heights stratified by father heights."}
galton_heights %>% 
  mutate(father = round(father)) %>% 
  group_by(father) %>%
  summarize(son_conditional_avg = mean(son)) %>%
  ggplot(aes(father, son_conditional_avg)) + 
  geom_point()
```

Stratification
===

```{r echo=FALSE, boxplot3, fig.cap="Boxplot of son heights stratified by father heights."}
galton_heights %>% 
  mutate(father = round(father)) %>% 
  group_by(father) %>%
  summarize(son_conditional_avg = mean(son)) %>%
  ggplot(aes(father, son_conditional_avg)) + 
  geom_point()
```

Stratification
===

- and the slope of this line appears to be about $0.5$ which happens to be the correlation between father and son heights.

- This is not a coincidence.

- To see the connection, let's plot the standardized heights with a line with slope equal to the correlation:

Stratification
===


```{r, eval=FALSE}
r <- galton_heights %>% summarize(r = cor(father, son)) %>% .$r
galton_heights %>% 
  mutate(father = round(father)) %>% 
  group_by(father) %>%
  summarize(son = mean(son)) %>%
  mutate(z_father = scale(father), z_son = scale(son)) %>%
  ggplot(aes(z_father, z_son)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = r)
```

Stratification
===

```{r, echo=FALSE}
r <- galton_heights %>% summarize(r = cor(father, son)) %>% .$r
galton_heights %>% 
  mutate(father = round(father)) %>% 
  group_by(father) %>%
  summarize(son = mean(son)) %>%
  mutate(z_father = scale(father), z_son = scale(son)) %>%
  ggplot(aes(z_father, z_son)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = r)
```

Stratification
===

- The line is what we call the *regression line*.

- In the next section we will describe Galton's theoretical justification for using this line to estimate conditional means.

- Here we define it and compute it for the data at hand. 

Stratification
===

- The regression line tells us that for every standard deviation, $\sigma_x$, increase above the average $mu_x$, $y$ grows $\rho$ standard deviations $\sigma_y$ above the average $\mu_y$.

 Stratification
===

- The formula for the regression line is therefore:

$$ 
\left( \frac{y_i-\mu_y}{\sigma_y} \right) = \rho \left( \frac{x_i-\mu_x}{\sigma_x} \right)
$$


Connection to Bivariate Normal
====

$$ 
\mbox{E}(Y | X=x) = \mu_Y +  \rho \frac{X-\mu_X}{\sigma_X}\sigma_Y
$$

- This is the regression line, with slope $$\rho \frac{\sigma_Y}{\sigma_X}$$ and intercept $\mu_y - m\mu_X$. It is equivalent to regression equation we showed earlier which can be written like this:


$$
\frac{\mbox{E}(Y \mid X=x)  - \mu_Y}{\sigma_Y} = \rho \frac{x-\mu_X}{\sigma_X}
$$


Variance explained
====

- The bivariate normal theory also tells us that the standard deviation of the _conditional_ distribution described above is:

$$
\mbox{SD}(Y \mid X=x ) = \sigma_Y \sqrt{1-\rho^2} 
$$



Correlation is not causation
===


```{r echo=FALSE}
library(tidyverse)
library(tidyr)
library(broom)
library(dslabs)
ds_theme_set()
```


- Correlation is not causation is perhaps the most important lesson one learns in a statistics class.

- In this chapter we have described tools useful for quantifying associations between variables.

- But we must be careful not to over interpret these association. 

- There are many reasons that a variable $X$ can correlated with a variable $Y$ without either being a cause for the other.

- Here we examine three common ways that can lead to misinterpreting data.

Spurious Correlation
===

- The following comical example underscores that correlation is not causation.

- The example shows a very strong correlation between divorce rates and margarine consumption. 


Spurious Correlation
===

```{r, echo=FALSE}
the_title <- paste("Correlation =", 
                round(with(divorce_margarine, 
                           cor(margarine_consumption_per_capita, divorce_rate_maine)),2))
data(divorce_margarine)
divorce_margarine %>% 
  ggplot(aes(margarine_consumption_per_capita, divorce_rate_maine)) + 
  geom_point(cex=3) + 
  geom_smooth(method = "lm") + 
  ggtitle(the_title) +
  xlab("Margarine Consumption per Capita (lbs)") + 
  ylab("Divorce rate in Maine (per 1000)")
```

Spurious Correlation
===

- Does this mean margarine causes divorces? Or do  divorces cause people to eat more margarine? Of course the answer to both these questions is no.

- This is just an example of what we call a _spurious correlation_.

- You can see many more absurd examples of [this wesbsite](http://tylervigen.com/spurious-correlations) completely dedicated to _spurious correaltions_. 

Spurious Correlation
===

- The cases presented in the _spurious correlation_ site are all examples of what is generally called _data dredging_, _data fishing_, or _data snooping_.

- It's basically a form of what in the US they call _cherry picking_.

- An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.

Spurious Correlation
===

- A Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables.

- We will save the results of our simulation into a tibble: 

```{r, cache=TRUE}
N <- 25
G <- 1000000
sim_data <- tibble(group = rep(1:G, each = N), X = rnorm(N*G), Y = rnorm(N*G))
```

Spurious Correlation
===

- The first columns denotes group and we simulated `r cat(prettyNum(G,big.mark=",",scientific=FALSE))` of groups each with `r N` observations.

- For each group we generate `r N` observations which are stored in the second and third columns.

- These are just random independent normally distributed data.

- So we know, because we constructed the simulation, that $X$ and $Y$ are not correlated.

Spurious Correlation
===

- Next, we compute the correlation between `X` and `Y` for each group and look at the max:

```{r}
res <- sim_data %>% 
  group_by(group) %>% 
  summarize(r = cor(X, Y)) %>% 
  arrange(desc(r))
```

Spurious Correlation
===

Here are the top correlations:

```{r}
res
```

Spurious Correlation
===

- We see a correlation of `r max(res$r)` and if you just plot the data from that group it shows a convincing plot that $X$ and $Y$ are in fact correlated:

```{r, echo=FALSE}
sim_data %>% filter(group == res$group[which.max(res$r)]) %>%
  ggplot(aes(X, Y)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

Spurious Correlation
===

- Remember that the correlation summary is a random variable.

- Here is the distribution generated by the Monte Carlo simulation:

Spurious Correlation
===

```{r, echo=FALSE}
res %>% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = "black")
```

Spurious Correlation
===

- It is just a mathematical fact that if we observe `r cat(prettyNum(G,big.mark=",",scientific=FALSE))` random correlations that are expected to be 0 but have a standard error of `r sd(res$r)`, the largest one will be close 1. 

- Note that if we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:

```{r}
sim_data %>% 
  filter(group == res$group[which.max(res$r)]) %>%
  do(tidy(lm(Y ~ X, data = .)))
```

Spurious Correlation
===

- This particular form of data dredging is referred to as _p-hacking_.

- P-hacking is a topic of much discussion because it 
is a problem in scientific publications.

- Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results.

Spurious Correlation
===

- In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and several exposures and report only the one exposure that resulted in a small p-value.

- Furthermore, they might try fitting several different models to adjust for confounding and pick the one that yields the smallest p-value.

- In experimental disciplines, an experiment might be repeated more than once and only the one that resulted in a small p-value reported.

- This does not necessarily happen due to unethical behavior, but rather to statistical ignorance or wishful thinking.

- In advanced statistics courses you learn methods to adjust for these multiple comparisons.

Outliers
===

- Suppose we take measurements from two independent outcomes, $X$ and $Y$ and we standardize the measurements.

- However, 
imagine we make a mistake and forget to standardize entry 23.

Outliers
===

- We can simulate such data using:


```{r}
set.seed(1)
x <- rnorm(100,100,1)
y <- rnorm(100,84,1)
x[-23] <- scale(x[-23])
y[-23] <- scale(y[-23])
```

Outliers
===

- The data look like this:

```{r}
tibble(x,y) %>% ggplot(aes(x,y)) + geom_point(alpha = 0.5)
```

Outliers
===

- Not surprisingly the correlation is very high:


```{r}
cor(x,y)
```

- But this is driven by the one outlier.

Outliers
===

- If we remove this outlier, the correlation is greatly reduced to almost 0, what it should be:


```{r}
cor(x[-23], y[-23])
```

Outliers
===

- There is a alternative to the sample correlation for estimating the population correlation that is robust to outliers.

- It is called _Spearman correlation_.

- The idea is simple: compute the correlation on the ranks of the values.


- Here is a plot of the ranks plotted against each other.

Outliers
===


```{r, echo=FALSE}
tibble(x,y) %>% 
  ggplot(aes(rank(x),rank(y))) + 
  geom_point(alpha = 0.5)
```

Outliers
===

- The outlier is no longer associated with a very large value and the correlation comes way down:


```{r}
cor(rank(x), rank(y))
```


Outliers
===

- Spearmen correlation can also be calculated like this:


```{r}
cor(x, y, method = "spearman")
```

- There are also methods for robust fitting of linear models which you can learn about in, for example, this book: Robust Statistics: Edition 2
Peter J. Huber Elvezio M. Ronchetti 

Reversing cause and effect
===

- Another way associations is confused with causation is when the cause and effect are reversed.

- An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored.

- Here the tutoring is not causing the low test scores but the other way around.  

Reversing cause and effect
===

- A form of this claim was actually made in an op-en in the New York Times titled [Parental Involvement Is Overrated](https://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated/?rref=opinion&module=ArrowsNav&contentCollection=Opinion&action=keypress&region=FixedLeft&pgtype=Blogs).

- Consider this quote from the article:

- >> When we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades... Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.

Reversing cause and effect
===

- A very likely possibility is that the kids needing regular parental help, get this help because they don't perform well in school.

- We can easily construct an example of cause and effect reversal using the father and son height data.

Reversing cause and effect
===

- Note that if we fit the model

$$X_i = \beta_0 + \beta_1 y_i + \varepsilon_i, i=1, \dots, N$$

- to the father and son height data, with $X_i$ the father height and $y_i$ the son height we do get a statistically significant result:

Reversing cause and effect
===

```{r}
library(HistData)
data("GaltonFamilies")
GaltonFamilies %>%
  filter(childNum == 1 & gender == "male") %>%
  select(father, childHeight) %>%
  rename(son = childHeight) %>% 
  do(tidy(lm(father ~ son, data = .)))
```

Reversing cause and effect
===

- The model fits the data very well.

- If we look at the mathematical formulation of the model, it could easily be incorrectly interpreted as to suggest that the son being tall caused the father to be tall.

- But given what we know about genetics and biology, we know it's the other way around.

Reversing cause and effect
===

- The model is technically correct.

- The estimates and p-values were obtained correctly as well.

- What is wrong here is the interpretation.

Confounders
===

- Confounders are perhaps the most common reason that leads to associations begin misinterpreted.  

- If $X$ and $Y$ are correlated, we call $Z$ a _confounder_ if changes in $Z$ causes changes in both $X$ and $Y$.

- Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs.

Confounders
===

- In some cases we can use linear models to account for confounders.

- But it is not always the case.

- Incorrect interpretation due to confounders are ubiquitous in the lay press.

- They are sometimes hard to detect.

- Here we present two examples, both related to gender discrimination.



Example: UC Berkeley Admissions 
===

- Admission data from six U.C.

- Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O'Connell.  Science (1975).


Example: UC Berkeley Admissions 
===

- Here is the data:


```{r}
data(admissions)
admissions
```


Example: UC Berkeley Admissions 
===

- The percent of men and women that got accepted are


```{r}
admissions %>% group_by(gender) %>% 
  summarize(percentage = 
              round(sum(admitted*applicants)/sum(applicants),1))
```

Example: UC Berkeley Admissions 
===

- A statistical test clearly rejects the hypothesis that gender and admission are independent:


```{r}
admissions %>% group_by(gender) %>% 
  summarize(total_admitted = round(sum(admitted/100*applicants)), 
            not_admitted = sum(applicants) - sum(total_admitted)) %>%
  select(-gender) %>% 
  do(tidy(chisq.test(.)))
```

Example: UC Berkeley Admissions 
===

- But closer inspection shows a paradoxical result.  Here are the percent admissions by major:


```{r}
admissions %>% select(major, gender, admitted) %>%
  spread(gender, admitted) %>%
  mutate(women_minus_men = women - men)
```

Example: UC Berkeley Admissions 
====

- Four out of the six majors favor women.

- More importantly all the difference are much smaller than the 14.2 difference that we see when examining the totals.

- The paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear.

- What's going on? This actually can happen if an uncounted  confounder  is driving most of the variability.

Example: UC Berkeley Admissions 
===

- So let's define three variables: $X$ is 1 for men and 0 for women, $Y$ is 1 for those admitted and 0 otherwise, and $Z$ quantifies how selective is  the major.

- A gender bias claim would be based on the fact that $\mbox{Pr}(Y=1 | X = x)$ is higher for $x=1$ then $x=0$.

- But $Z$ is an important confounder to consider.

- Clearly $Z$ is associated with  $Y$, as the more selective a major the lower $\mbox{Pr}(Y=1 | Z = z)$.

- But is major selectivity $Z$ associated with gender $X$?

Example: UC Berkeley Admissions 
===

- One way to see this is to plot the total percent admitted to a major versus the percent of women that make up the applicants:

```{r, echo=FALSE}
admissions %>% 
  group_by(major) %>% 
  summarize(major_selectivity = sum(admitted*applicants)/sum(applicants),
            percent_women_applicants = sum(applicants*(gender=="women")/sum(applicants))*100) %>%
  ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +
  geom_text()
```


Example: UC Berkeley Admissions 
===

```{r, echo=FALSE}
admissions %>% 
  group_by(major) %>% 
  summarize(major_selectivity = sum(admitted*applicants)/sum(applicants),
            percent_men_applicants = sum(applicants*(gender=="men")/sum(applicants))*100) %>%
  ggplot(aes(major_selectivity, percent_men_applicants, label = major)) +
  geom_text()
```

Simpson's Paradox 
===

- Here is an illustrative example.

- Suppose you have three variables $X$, $Y$ and $Z$.

- Here is a scatterplot of $Y$ versus $X$:


Simpson's Paradox 
====

```{r, echo=FALSE}
N <- 100
Sigma <- matrix(c(1,0.75,0.75, 1), 2, 2)*1.5
means <- list(c(11,3), c(9,5), c(7,7), c(5,9), c(3,11))
dat <- lapply(means, function(mu) 
  MASS::mvrnorm(N, mu, Sigma))
dat <- tbl_df(Reduce(rbind, dat)) %>% 
  mutate(Z = as.character(rep(seq_along(means), each = N)))
names(dat) <- c("X", "Y", "Z")
dat %>% ggplot(aes(X,Y)) + geom_point(alpha = .5) +
  ggtitle(paste("correlation = ", round(cor(dat$X, dat$Y), 2)))
```

Simpson's Paradox 
====
- You can see that $X$ and $Y$ are negatively correlated.

- However, once we stratify by $Z$ (shown in different colors), which we have not yet shown, another pattern emerges. 


Simpson's Paradox 
====

```{r, echo=FALSE}
means <- tbl_df(Reduce(rbind, means)) %>% setNames(c("x","y")) %>%
  mutate(z = as.character(seq_along(means)))
  
corrs <- dat %>% group_by(Z) %>% summarize(cor = cor(X,Y)) %>% .$cor 

dat %>% ggplot(aes(X, Y, color = Z)) + 
  geom_point(show.legend = FALSE, alpha = 0.5) +
  ggtitle(paste("correlations =",  paste(signif(corrs,2), collapse=" "))) +
  annotate("text", x = means$x, y = means$y, label = paste("Z=", means$z), cex = 5)  
```

Simpson's Paradox 
====

- It is really $Z$ that is negatively correlated with $X$.

- If we stratify by $Z$ the $X$ and $Y$ are actually positively correlated:

